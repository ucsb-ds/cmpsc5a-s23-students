{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad1fdc5-9243-4aac-ad39-ab0c6bbf229d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247ca4c2-0858-4007-b97d-3f3109c0fb45",
   "metadata": {},
   "source": [
    "# Lecture 19 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dad4f6-2ff7-4b98-988a-463f9bc7957d",
   "metadata": {},
   "source": [
    "## Do people with pets get more exercise?\n",
    "\n",
    "### Preparing the Data\n",
    "\n",
    "Let's load the class dataset again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1cd351-80bf-46fb-a90a-bbaf25e5ba63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_data_w23 = Table.read_table('data/cmpsc5a-classdata-w23.csv')\n",
    "class_data_w23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7221a7e6-d064-4bec-a9c5-ab7e3f42da2f",
   "metadata": {},
   "source": [
    "## Subsetting\n",
    "There's a lot of data here that isn't relevant to our research question, so we can clean this up somewhat by selecting only the columns indicating number of pets and hours of exercise:\n",
    "\n",
    "**Question:** what table method can I use to get a table with only the Pets and Exercise columns?\n",
    "\n",
    "1. `column`\n",
    "1. `select`\n",
    "1. `take`\n",
    "1. `with_column`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efaa6b9-4d97-4cac-87ab-51cb03916fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pets_exercise_w23 = ...\n",
    "pets_exercise_w23.show(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2f157c-6f79-4068-9ad0-8550b0b2eff6",
   "metadata": {},
   "source": [
    "There's still some more cleaning to do! Not everyone answered both of these questions, leading to values of `nan` in the table where responses are missing. We can further clean up the data by selecting only the rows where both `Pets` and `Exercise` are a number that is at least zero.\n",
    "\n",
    "**Question:** if I only want to keep rows where the values in a column satisfy some condition, which table method should I use?\n",
    "\n",
    "1. `sort`\n",
    "1. `drop`\n",
    "1. `exclude`\n",
    "1. `where`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d053f7bb-3119-4905-a86a-c8e2e538a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the table to rows where Pets >= 0\n",
    "pets_exercise_w23 = ...\n",
    "\n",
    "# Further filter the table to rows where Exercise >= 0\n",
    "pets_exercise_w23 = ...\n",
    "\n",
    "pets_exercise_w23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e6dc70-7d2d-419d-bc5f-a7fac3c34c96",
   "metadata": {},
   "source": [
    "The rows containing any `nan` values should be gone. Since `nan` is not a number, it will not be allowed by our predicate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89995f60-6165-4dee-9aa7-8a8725c46eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can treat the predicate like a function to see if nan is considered above or equal to 0\n",
    "predicate = are.above_or_equal_to(0)\n",
    "predicate(float('nan'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf8efd-c2cd-4b6c-b760-0c3c7908358b",
   "metadata": {},
   "source": [
    "There are quite a few responses left! Let's see exactly how many.\n",
    "\n",
    "**Question:** if I have a table `t` representing a dataset, how do I count how many individuals are in the dataset?\n",
    "1. `t.columns`\n",
    "1. `t.rows`\n",
    "1. `t.num_columns`\n",
    "1. `t.num_rows`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0909b0-a3f3-4f46-a364-a3495f65b64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many respondents are left after cleaning\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ccd47c-7702-4db0-a8b7-85efcfe74107",
   "metadata": {},
   "source": [
    "### Exploring the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddff1584-cdc0-45c1-8533-0923fb3def55",
   "metadata": {},
   "source": [
    "When you have a new dataset, it's a good idea to use some summary statistics and visualizations to get familiar with the data. We can make a couple of histograms to get a sense of responses for numbers of pets and hours of exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436619f3-f5f5-4579-85b8-c7ef363bebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pet_bins = np.arange(-0.5, 6) # Create bins so that 0, 1, 2, ..., 5 are in the center\n",
    "pets_exercise_w23.hist('Pets', bins=pet_bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4afc599-8c4b-48c7-a779-6e1128958b84",
   "metadata": {},
   "source": [
    "**Question:** based on the histogram, about what fraction of the class has at least one pet?\n",
    "\n",
    "1. 50%\n",
    "1. 60%\n",
    "1. 70%\n",
    "1. 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91094769-2b25-4ae5-804f-09c9eb16a852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffe4dabb-8389-4cf8-9ce3-096541f0afbb",
   "metadata": {},
   "source": [
    "We can calculate exactly what fraction of the class has at least one pet. There are several ways to do it!\n",
    "\n",
    "**Question:** which of the following expressions does *not* calculate the fraction of the class that has at least one pet?\n",
    "1. `sum(pets_exercise_w23.column('Pets') >= 1) / 75`\n",
    "1. `sum(pets_exercise_w23.column('Pets') > 0)`\n",
    "1. `np.count_nonzero(pets_exercise_w23.column('Pets') > 0) / pets_exercise_w23.num_rows`\n",
    "1. `np.mean(pets_exercise_w23.column('Pets') >= 1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea635f48-75e5-465a-ae85-3b541554f374",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7585d86-3754-44c8-ac2a-0f6555dda5ec",
   "metadata": {},
   "source": [
    "To check for an association between these variables, it's also a good idea to try a scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27a0c1a-0f57-410a-b87e-d42d2329122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pets_exercise_w23.scatter('Pets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95be114-49f6-404f-bc2e-9466cb44bd23",
   "metadata": {},
   "source": [
    "Does the plot indicate any kind of association? Maybe---it looks like exercise may weakly trend upward with number of pets. We haven't proven anything yet, but we have found some evidence to motivate further analysis of this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76abb8c-b158-4444-a5a7-627d546f9107",
   "metadata": {},
   "source": [
    "For one more visualization, let's create a column called `at_least_one_pet` indicating whether or not each respondent has any pet at all. There are a couple of ways to do this. We could define a function and evaluate it on each row of the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ce86e-0d17-454a-a313-6f1de4ad846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def at_least_one(x):\n",
    "    \"\"\"\n",
    "    Returns True if x >= 1; otherwise returns False.\n",
    "    \"\"\"\n",
    "    return x >= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae94d04c-5c85-43bc-93f7-d3f018b39077",
   "metadata": {},
   "source": [
    "**Question:** if I want an array containing the values `at_least_one(x)` for each number `x` in the Pets column, which table method should I use?\n",
    "\n",
    "1. `with_column`\n",
    "1. `take`\n",
    "1. `apply`\n",
    "1. `group`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e5b85c-ce61-4495-87da-1f9c390ec627",
   "metadata": {},
   "outputs": [],
   "source": [
    "at_least_one_pet = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41352e56-54d9-47d2-8154-e7d2723fad54",
   "metadata": {},
   "source": [
    "But this calculation is simple enough that we can just work with the `Pets` column directly, as an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45f5b27-7dd7-4c07-b7e3-343605dd1300",
   "metadata": {},
   "outputs": [],
   "source": [
    "at_least_one_pet = pets_exercise_w23.column('Pets') >= 1\n",
    "at_least_one_pet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3202885b-a851-4f45-a170-df046648f28a",
   "metadata": {},
   "source": [
    "If we can avoid using the apply method, this is usually better---working with arrays directly is faster for the computer! But however we compute this array, we can add it as a column to our table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8e9d4a-cbba-4732-9a3e-cbf2fe01216a",
   "metadata": {},
   "source": [
    "**Question:** what statement should I use to permanently add the `at_least_one_pet` array as a new column to the table?\n",
    "\n",
    "1. `pets_exercise_w23.with_column('at_least_one_pet', at_least_one_pet)`\n",
    "1. `pets_exercise_w23 = pets_exercise_w23.with_column('at_least_one_pet', at_least_one_pet)`\n",
    "1. `pets_exercise_w23.column('at_least_one_pet') = at_least_one_pet`\n",
    "1. Trick question: due to the Law of Impermanence, everything that is material is also temporary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d86ae6b-4903-4a6a-aa45-ea05d314b6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the at_least_one_pet column to the table\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7236143b-348c-4627-a003-8fc70507fa46",
   "metadata": {},
   "source": [
    "Now let's make one last visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1abded6-19ae-46a1-b3dd-332c27f03d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "pets_exercise_w23.hist('Exercise', group='at_least_one_pet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9188f1-2b41-4413-a9fc-602f6e8f81cf",
   "metadata": {},
   "source": [
    "This pair of histograms agrees with our earlier idea: that there may be some positive correlation between pet ownership and amount of exercise. Let's see what the average amount of exercise is for these two groups.\n",
    "\n",
    "**Question:** I want to calculate the average value of Exercise for the group where `at_least_one_pet` is True and the group where it is False. What table method should I use?\n",
    "1. `where`\n",
    "2. `group`\n",
    "3. `pivot`\n",
    "4. `apply`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b5c387-3070-4270-9f29-990a8dc77caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_means = ...\n",
    "group_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596ff0a5-8561-4826-bec7-c869ca50e5cd",
   "metadata": {},
   "source": [
    "On average, survey respondents who have a pet get 2 more hours of exercise per week than those who do not. *This could just be due to random chance.* Maybe, by pure coincidence, highly active pet owners just happened to respond to the survey more often. We can't make any conclusions until we understand how likely it is that this result is due to chance alone.\n",
    "\n",
    "### Formulating a Hypothesis\n",
    "\n",
    "Based on what we have seen, we suspect that people who have a pet get more exercise (on average) than people who do not have a pet. To see if our data supports this claim, we need to do a hypothesis test.\n",
    "\n",
    "In this case, the **null hypothesis** is that there is no difference between the distributions of exercise between the two groups. In other words, if we were able to somehow get exercise and pet ownership data on the entire population, the blue and yellow histograms above would be identical. The null hypothesis is the model in which any discrepancy in exercise amount between the two groups is entirely due to random chance in who responded to the survey.\n",
    "We have a couple of choices for an alternative hypothesis, depending on what claims we want to assess.\n",
    "\n",
    "\n",
    " - Option 1: the two distributions are different. Maybe people who have pets get more exercise, maybe they get less exercise, but regardless of which of these is true, the distributions aren't identical. \n",
    " - Option 2: the distributions are different, and people who have pets get more exercise on average. \n",
    " - Option 3: the distributions are different, and people who have pets get less exercise on average.\n",
    " \n",
    " From our exploratory analysis, we suspect that Option 2 is true, so we will choose this to be our alternative hypothesis:\n",
    " \n",
    " **Alternative Hypothesis:** people who have pets get more exercise on average.\n",
    " \n",
    " ### Testing the Hypothesis\n",
    "**Question:** if our null hypothesis is that two samples come from the same distribution, how do we decide if there is enough statistical evidence to disprove this claim?\n",
    "\n",
    "1. Plot the histograms for each group and see if they look different\n",
    "1. Check if the group means are close together or not\n",
    "1. Calculate the TVD between the two samples\n",
    "1. A/B Test\n",
    "\n",
    "For this test, we will use the difference in means as the test statistic. The value of this statistic is `group_b_mean - group_a_mean`. In this case, Group A are students who do not have pets, and Group B are students who own at least one pet. With this in mind, notice that *larger values of the test statistic support the alternative hypothesis*.\n",
    "\n",
    "We can use the following function from last class to calculate the difference in group means from a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3c704b-401e-4358-aefb-0c6530f77ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_of_means(table, label, group_label):\n",
    "    \"\"\"\n",
    "    Calculates the difference in means between two groups of rows in a table.\n",
    "    Takes a table, the name of a column representing a numerical variable,\n",
    "    and the name of a column indicating which group each row belongs to.\n",
    "    \"\"\"\n",
    "    \n",
    "    # table with the two relevant columns\n",
    "    reduced = table.select(label, group_label)  \n",
    "    \n",
    "    # table containing group means\n",
    "    means_table = reduced.group(group_label, np.average)\n",
    "    \n",
    "    # array of group means\n",
    "    means = means_table.column(1)\n",
    "    \n",
    "    return means.item(1) - means.item(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de94cfa0-48b0-41ae-b39c-faf4040ba065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the observed value of the test statistic\n",
    "observed_test_stat = difference_of_means(pets_exercise_w23, 'Exercise', 'at_least_one_pet')\n",
    "observed_test_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3b3e1b-688b-4cb1-a7e2-b605a6b1a3b4",
   "metadata": {},
   "source": [
    "### Simulating the Null Hypothesis\n",
    "\n",
    "We will now simulate the values of the test statistic under the null hypothesis.\n",
    "\n",
    "Simulating the null hypothesis for A/B testing (i.e., that two samples came from the same distribution) is not as straightforward as simulating the null hypotheses for one-sample tests that we have seen. We haven't specified *what* this distribution is: only that it's the same for these two groups. Therefore, we can't just use `sample_proportions` or `np.random.choice`, since we don't know what probabilities to assign to each outcome.\n",
    "\n",
    "A/B tests get around this problem using a clever trick: we can get new samples of this distribution by *resampling* our data, i.e., creating new samples from our *data* instead of from some probability distribution.\n",
    "\n",
    "To simulate our dataset under the null hypothesis, we want to draw two samples, one from Group A and one from Group B. The null hypothesis is that these samples should come from the same distribution. Therefore, we can pool all of our data---from both groups---into one larger sample from whatever this distribution is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa9e73-aa04-40d1-a778-99065b4f4076",
   "metadata": {},
   "outputs": [],
   "source": [
    "pets_exercise_w23.hist('Exercise', group='at_least_one_pet')\n",
    "plots.title('Distributions from our Separate Samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a89f53-f173-4090-bfb3-b97654bcd7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pets_exercise_w23.hist('Exercise')\n",
    "plots.title('Distribution of our Combined Sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b060f3a0-4d2a-4173-8cbf-00c85fd76e6a",
   "metadata": {},
   "source": [
    "If the two groups are from the same distribution, then *the group label doesn't matter*. If you have two coins that you know are fair, it doesn't matter which one you flip: you have the same likelihood of landing on heads. Similarly, if students with pets and students without pets exercise according to the same distribution, group membership doesn't have any impact on how much exercise a student is likely to get.\n",
    "\n",
    "In order to get the distribution of our test statistic under the null hypothesis, we first need to repeatedly simulate the dataset that we have available---in other words, simulate survey responses for the group with pets and the group without pets. We can do this by randomly selecting our simulated \"Group A\" responses from the combined distribution (without replacement), and leaving the remaining responses in our simulated \"Group B\". An equivalent (but easier) way to implement this in code is to randomly shuffle the order of the group labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d35bb7-7cc1-48a1-ada9-1f3e949981c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Randomly shuffle (i.e., re-order) the group labels\n",
    "# This preserves the original size of each group\n",
    "shuffled_group_label = pets_exercise_w23.sample(with_replacement=False).column('at_least_one_pet')\n",
    "shuffled_group_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b53adc4-6190-4cd9-8129-ab810c7e3275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add the suffled labels to a table with the values of Exercise in their original order\n",
    "# In effect, we have randomly sampled Group A and Group B from the combined distribution of Exercise\n",
    "shuffled_table = Table().with_columns(\n",
    "        'Exercise', pets_exercise_w23.column('Exercise'),\n",
    "        'Group', shuffled_group_label)\n",
    "shuffled_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c667120-1738-48ca-9d50-42e28efeceb2",
   "metadata": {},
   "source": [
    "Then it's easy to compute the simulated value of our test statistic, using the `difference_of_means` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f451390-4ced-4f89-a313-e7516a0138d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_of_means(shuffled_table, 'Exercise', 'Group')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea916c1-7896-4024-871e-2b63fd277b72",
   "metadata": {},
   "source": [
    "As usual, we will repeat this simulation process many times, constructing an array of simulated test statistics using the `np.append` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984ccebd-a8e7-4a06-a42d-88bcefe845c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_test_stats = make_array()\n",
    "\n",
    "for i in range(2000):\n",
    "\n",
    "    # Shuffle the group labels\n",
    "    shuffled_group_label = pets_exercise_w23.sample(with_replacement=False).column('at_least_one_pet')\n",
    "\n",
    "    # Make a table with original exercise values and shuffled group labels\n",
    "    shuffled_table = Table().with_columns(\n",
    "        'Exercise', pets_exercise_w23.column('Exercise'),\n",
    "        'Group', shuffled_group_label)\n",
    "    \n",
    "    # Calculate the test statistic\n",
    "    group_means = shuffled_table.group('Group', np.mean)\n",
    "    simulated_test_stat = group_means.column(1).item(1) - group_means.column(1).item(0)\n",
    "    \n",
    "    simulated_test_stats = np.append(simulated_test_stats, simulated_test_stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1219572-7186-44ce-9b7e-3e27571e2669",
   "metadata": {},
   "source": [
    "Finally, we plot a histogram of the distribution of test statistics under the null hypothesis, and we compare it our observed value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e50b9d-a749-417d-bd62-3816da558be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Table().with_column('Test Stat Under the Null', simulated_test_stats).hist()\n",
    "\n",
    "# Mark the observed value of the test statistic\n",
    "plots.scatter(observed_test_stat, 0, color='red', s=40)\n",
    "plots.ylim([-0.05, 0.5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44848139-34a5-47e9-b6be-b52afbf7f137",
   "metadata": {},
   "source": [
    "**Question:** how can I estimate the $p$-value of this test?\n",
    "\n",
    "1. Since positive values of the test statistic support the alternative hypothesis, calculate the fraction of simulated test statistics that are negative.\n",
    "1. Since positive values of the test statistic support the alternative hypothesis, calculate the fraction of simulated test statistics that are positive.\n",
    "1. Calculate the fraction of simulated test statistics that are *at least* as large as the observed value.\n",
    "1. Calculate the fraction of simulated test statistics that are *at most* as large as the observed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d704063-610b-4238-b8e1-faf6d9543d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the p-value\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88e4a14-cd8e-4d6c-a9d5-90c3655cb458",
   "metadata": {},
   "source": [
    "**Question:** what is the outcome of this test?\n",
    "\n",
    "1. We fail to reject the null hypothesis. \n",
    "1. We reject the null hypothesis (the result is statistically significant).\n",
    "1. We reject the null hypothesis (the result is highly statistically significant).\n",
    "1. We have proven the null hypothesis to be true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e3c9b9-88e0-48b9-97f9-7d8cb051b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c9a2c5-3e30-406e-b365-06bbd9759668",
   "metadata": {},
   "source": [
    "**Question:** Suppose that our statistics journal only accepts highly statistically significant results. Which of the following is *not* true?\n",
    "\n",
    "1. We require the area of the tail to be less than 1% in order to reject the null hypothesis.\n",
    "1. At a significance threshold of 0.01, if the null hypothesis is correct, there is still a 1% chance that we will (incorrectly) reject the null hypothesis.\n",
    "1. If we have published 100 statistically significant results, it is possible that one or two of them are actually incorrect, even if the hypothesis test was done correctly ([relevant xkcd](https://xkcd.com/882/)).\n",
    "1. If $p = 0.01$, there is a 1% chance that the null hypothesis is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97251840-7b50-448e-a13c-42cd30038ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
